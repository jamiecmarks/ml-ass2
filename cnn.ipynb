{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_meta = pd.read_csv(\"./train/train_metadata.csv\")\n",
    "train_add_features = pd.read_csv(\"./train/features/additional_features.csv\")\n",
    "train_color_hist = pd.read_csv(\"./train/features/color_histogram.csv\")\n",
    "train_hog_pca = pd.read_csv(\"./train/features/hog_pca.csv\")\n",
    "\n",
    "\n",
    "test_meta = pd.read_csv(\"./test/test_metadata.csv\")\n",
    "# test_meta.drop(\"ClassId\", inplace=True, axis=1) # useless for now\n",
    "test_add_features = pd.read_csv(\"./test/features/additional_features.csv\")\n",
    "test_color_hist = pd.read_csv(\"./test/features/color_histogram.csv\")\n",
    "test_hog_pca = pd.read_csv(\"./test/features/hog_pca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging all the dataframes\n",
    "\n",
    "train_df = pd.merge(train_meta, train_add_features, on = \"image_path\", how = \"left\")\n",
    "train_df = pd.merge(train_df, train_color_hist, on = \"image_path\", how = \"left\")\n",
    "train_df = pd.merge(train_df, train_hog_pca, on = \"image_path\", how = \"left\")\n",
    "\n",
    "test_df = pd.merge(test_meta, test_add_features, on = \"image_path\", how = \"left\")\n",
    "test_df = pd.merge(test_df, test_color_hist, on = \"image_path\", how = \"left\")\n",
    "test_df = pd.merge(test_df, test_hog_pca, on = \"image_path\", how = \"left\")\n",
    "\n",
    "\n",
    "# this is just better for reading files\n",
    "train_df[\"image_path\"] = train_df[\"image_path\"].apply(lambda x : \"train/\" + x)\n",
    "test_df[\"image_path\"] = test_df[\"image_path\"].apply(lambda x : \"test/\" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[[\"image_path\", \"ClassId\", \"id\"]]\n",
    "\n",
    "test_df = test_df[[\"image_path\", \"ClassId\", \"id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duncanjcartwright/Documents/uni_stuff/year3/ML/assignments/ass2/tf-metal/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.1335 - loss: 4.1946\n",
      "Epoch 1: val_accuracy improved from -inf to 0.02004, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 242ms/step - accuracy: 0.1340 - loss: 4.1905 - val_accuracy: 0.0200 - val_loss: 4.0999 - learning_rate: 5.0000e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.4287 - loss: 2.5934\n",
      "Epoch 2: val_accuracy improved from 0.02004 to 0.02732, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 244ms/step - accuracy: 0.4291 - loss: 2.5921 - val_accuracy: 0.0273 - val_loss: 3.8108 - learning_rate: 5.0000e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.5960 - loss: 1.9801\n",
      "Epoch 3: val_accuracy improved from 0.02732 to 0.12659, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 243ms/step - accuracy: 0.5962 - loss: 1.9795 - val_accuracy: 0.1266 - val_loss: 3.2571 - learning_rate: 5.0000e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.7087 - loss: 1.6546\n",
      "Epoch 4: val_accuracy improved from 0.12659 to 0.59290, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 243ms/step - accuracy: 0.7088 - loss: 1.6544 - val_accuracy: 0.5929 - val_loss: 1.9254 - learning_rate: 5.0000e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8076 - loss: 1.4240\n",
      "Epoch 5: val_accuracy improved from 0.59290 to 0.81785, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 242ms/step - accuracy: 0.8076 - loss: 1.4239 - val_accuracy: 0.8179 - val_loss: 1.3510 - learning_rate: 5.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8407 - loss: 1.3067\n",
      "Epoch 6: val_accuracy improved from 0.81785 to 0.88707, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 242ms/step - accuracy: 0.8407 - loss: 1.3067 - val_accuracy: 0.8871 - val_loss: 1.1543 - learning_rate: 5.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8835 - loss: 1.2227\n",
      "Epoch 7: val_accuracy improved from 0.88707 to 0.90346, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 242ms/step - accuracy: 0.8834 - loss: 1.2228 - val_accuracy: 0.9035 - val_loss: 1.1248 - learning_rate: 5.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8990 - loss: 1.1774\n",
      "Epoch 8: val_accuracy did not improve from 0.90346\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 240ms/step - accuracy: 0.8990 - loss: 1.1774 - val_accuracy: 0.9016 - val_loss: 1.1128 - learning_rate: 5.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.9224 - loss: 1.1226\n",
      "Epoch 9: val_accuracy improved from 0.90346 to 0.91257, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 241ms/step - accuracy: 0.9224 - loss: 1.1226 - val_accuracy: 0.9126 - val_loss: 1.0964 - learning_rate: 5.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.9242 - loss: 1.1038\n",
      "Epoch 10: val_accuracy improved from 0.91257 to 0.93352, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 243ms/step - accuracy: 0.9242 - loss: 1.1038 - val_accuracy: 0.9335 - val_loss: 1.0508 - learning_rate: 5.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.9475 - loss: 1.0700\n",
      "Epoch 11: val_accuracy did not improve from 0.93352\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 240ms/step - accuracy: 0.9474 - loss: 1.0700 - val_accuracy: 0.9335 - val_loss: 1.0356 - learning_rate: 5.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.9510 - loss: 1.0460\n",
      "Epoch 12: val_accuracy improved from 0.93352 to 0.93898, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 242ms/step - accuracy: 0.9510 - loss: 1.0461 - val_accuracy: 0.9390 - val_loss: 1.0416 - learning_rate: 5.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.9519 - loss: 1.0466\n",
      "Epoch 13: val_accuracy improved from 0.93898 to 0.93989, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 241ms/step - accuracy: 0.9519 - loss: 1.0466 - val_accuracy: 0.9399 - val_loss: 1.0196 - learning_rate: 5.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.9625 - loss: 1.0138\n",
      "Epoch 14: val_accuracy improved from 0.93989 to 0.94353, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 243ms/step - accuracy: 0.9624 - loss: 1.0138 - val_accuracy: 0.9435 - val_loss: 0.9992 - learning_rate: 5.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.9584 - loss: 1.0192\n",
      "Epoch 15: val_accuracy improved from 0.94353 to 0.95173, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 242ms/step - accuracy: 0.9585 - loss: 1.0191 - val_accuracy: 0.9517 - val_loss: 0.9886 - learning_rate: 5.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.9699 - loss: 0.9931\n",
      "Epoch 16: val_accuracy did not improve from 0.95173\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 241ms/step - accuracy: 0.9699 - loss: 0.9931 - val_accuracy: 0.9508 - val_loss: 0.9847 - learning_rate: 5.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.9775 - loss: 0.9863\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.95173\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 240ms/step - accuracy: 0.9775 - loss: 0.9863 - val_accuracy: 0.9463 - val_loss: 1.0019 - learning_rate: 5.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.9811 - loss: 0.9675\n",
      "Epoch 18: val_accuracy did not improve from 0.95173\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 241ms/step - accuracy: 0.9810 - loss: 0.9675 - val_accuracy: 0.9499 - val_loss: 0.9794 - learning_rate: 1.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.9718 - loss: 0.9789\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.95173\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 240ms/step - accuracy: 0.9718 - loss: 0.9788 - val_accuracy: 0.9499 - val_loss: 0.9715 - learning_rate: 1.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.9799 - loss: 0.9669\n",
      "Epoch 20: val_accuracy did not improve from 0.95173\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 242ms/step - accuracy: 0.9799 - loss: 0.9669 - val_accuracy: 0.9517 - val_loss: 0.9693 - learning_rate: 2.0000e-06\n",
      "Epoch 20: early stopping\n",
      "Restoring model weights from the end of the best epoch: 15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9488 - loss: 0.9974\n",
      "Validation accuracy: 0.9517\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step\n"
     ]
    }
   ],
   "source": [
    "# train a CNN to classify the images into classes\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Define image dimensions - smaller for faster training\n",
    "IMG_HEIGHT, IMG_WIDTH = 128, 128\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_images(image_paths, img_height=IMG_HEIGHT, img_width=IMG_WIDTH):\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        img = cv2.imread(path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (img_width, img_height))\n",
    "            img = img / 255.0  # Normalize\n",
    "            images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Get image paths and labels from train_df\n",
    "image_paths = train_df['image_path'].values\n",
    "labels = train_df['ClassId'].values\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 43\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "# Load images\n",
    "X_images = load_images(image_paths)\n",
    "\n",
    "# Split data\n",
    "X_train_img, X_val_img, y_train_img, y_val_img = train_test_split(\n",
    "    X_images, labels, test_size=0.2, random_state=42, stratify=np.argmax(labels, axis=1)\n",
    ")\n",
    "\n",
    "def mixup(x, y, alpha=0.2):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = np.random.permutation(len(x))\n",
    "    x_mix = lam * x + (1 - lam) * x[idx]\n",
    "    y_mix = lam * y + (1 - lam) * y[idx]\n",
    "    return x_mix, y_mix\n",
    "\n",
    "# Data augmentation with fewer transformations for speed\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Build a custom CNN model\n",
    "def build_cnn_model(num_classes=43):\n",
    "    model = models.Sequential([\n",
    "        # First Convolutional Block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        # Third Convolutional Block\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        # Dense Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "cnn_model = build_cnn_model()\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
    "\n",
    "INITIAL_LR = 0.00005\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.00005),\n",
    "    loss=loss,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Add callbacks\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_cnn_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Create a custom data generator that applies mixup\n",
    "class MixupDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x, y, batch_size=32, alpha=0.2):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        # Apply mixup to the batch\n",
    "        batch_x, batch_y = mixup(batch_x, batch_y, self.alpha)\n",
    "        return batch_x, batch_y\n",
    "\n",
    "# Create the mixup data generator\n",
    "train_generator = MixupDataGenerator(X_train_img, y_train_img, batch_size=32)\n",
    "\n",
    "# Train model with mixup\n",
    "history = cnn_model.fit(\n",
    "    X_train_img, y_train_img,\n",
    "    validation_data=(X_val_img, y_val_img),\n",
    "    epochs=50,\n",
    "    callbacks=[reduce_lr, early_stopping, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Load the best model saved during training\n",
    "cnn_model = tf.keras.models.load_model('best_cnn_model.h5')\n",
    "\n",
    "# Evaluate model\n",
    "val_loss, val_acc = cnn_model.evaluate(X_val_img, y_val_img)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Make predictions on test set\n",
    "test_image_paths = test_df['image_path'].values\n",
    "X_test_img = load_images(test_image_paths)\n",
    "cnn_predictions = cnn_model.predict(X_test_img)\n",
    "cnn_pred_classes = np.argmax(cnn_predictions, axis=1)\n",
    "\n",
    "# Save CNN predictions\n",
    "cnn_pred_df = pd.DataFrame({'id': test_df['id'], 'ClassId': cnn_pred_classes})\n",
    "cnn_pred_df.set_index('id', inplace=True)\n",
    "cnn_pred_df.to_csv('cnn_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
