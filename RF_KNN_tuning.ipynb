{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Custom dataset made by my partner\n",
    "train_dataset = pd.read_csv(\"custom_training_data.csv\")\n",
    "test_dataset = pd.read_csv(\"custom_test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dataset.drop(columns=[\"Unnamed: 0\", \"image_path\", \"id\"], axis=1)\n",
    "test_df = test_dataset.drop(columns=[\"Unnamed: 0\", \"image_path\", \"id\", \"ClassId\"], axis=1)\n",
    "\n",
    "# Remove all columns starting with \"hsv_bin\"\n",
    "# train_df = train_df.loc[:, ~train_df.columns.str.startswith(\"hsv_bin\")]\n",
    "# test_df = test_df.loc[:, ~test_df.columns.str.startswith(\"hsv_bin\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "IMAGE_SIZE = 96\n",
    "\n",
    "def load_images(paths):\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        image = Image.open(path)\n",
    "        images.append(image)\n",
    "    return images\n",
    "\n",
    "# Get RGB color contrast\n",
    "def get_color_contrast(images, width=IMAGE_SIZE, height=IMAGE_SIZE):\n",
    "    data = []\n",
    "\n",
    "    for image in images:\n",
    "        # Crop 3 pixels from each side\n",
    "        # img = img.crop((3, 3, img.width - 3, img.height - 3))\n",
    "\n",
    "        image = image.resize((width, height), Image.BILINEAR)\n",
    "\n",
    "        # Get normalised pixel values\n",
    "        image_data = np.asarray(image).astype(\"float32\") / 255.0\n",
    "        r, g, b = image_data[..., 0], image_data[..., 1], image_data[..., 2]\n",
    "\n",
    "        # Use ratio to better emulate the perceived brightness, instead of just doing the average\n",
    "        # https://en.wikipedia.org/wiki/Luma_(video)\n",
    "        luma = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "        red_green   = r - g\n",
    "        blue_yellow = b - (r + g) / 2.0\n",
    "\n",
    "        data.append(np.dstack((luma, red_green, blue_yellow)))\n",
    "\n",
    "    return np.array(data)\n",
    "\n",
    "# Get HSV color contrast\n",
    "def get_hsv_features(images, width=IMAGE_SIZE, height=IMAGE_SIZE):\n",
    "    data = []\n",
    "\n",
    "    for image in images:\n",
    "        image = image.resize((width, height), Image.BILINEAR)\n",
    "\n",
    "        # Convert RGB to HSV\n",
    "        rgb = np.asarray(image).astype(\"uint8\")\n",
    "        hsv = cv2.cvtColor(rgb[..., ::-1], cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Normalise HSV\n",
    "        hsv = hsv.astype(\"float32\") / 255.0\n",
    "        data.append(hsv)\n",
    "\n",
    "    return np.array(data)\n",
    "\n",
    "# Bin values chunk by chunk\n",
    "def joint_bin_features_hsv(image_features, num_bins=3, grid_size=4):\n",
    "    # Binned value per chunk\n",
    "    binned_features = []\n",
    "    \n",
    "    for image_feature in image_features:\n",
    "        chunk_h = image_feature.shape[0] // grid_size\n",
    "        chunk_w = image_feature.shape[1] // grid_size\n",
    "\n",
    "        features = []\n",
    "\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                chunk = image_feature[\n",
    "                    i * chunk_h:(i + 1) * chunk_h,\n",
    "                    j * chunk_w:(j + 1) * chunk_w,\n",
    "                    :\n",
    "                ]\n",
    "\n",
    "                # Clip and split to bins\n",
    "                chunk = np.clip(chunk, 0.0, 1.0)\n",
    "                bin_indices = np.floor(chunk * num_bins).astype(int)\n",
    "                bin_indices = np.clip(bin_indices, 0, num_bins - 1)\n",
    "\n",
    "                # Flatten into joint bin index for HSV channels\n",
    "                bin_h = bin_indices[..., 0]\n",
    "                bin_s = bin_indices[..., 1]\n",
    "                bin_v = bin_indices[..., 2]\n",
    "                combined = bin_h * (num_bins ** 2) + bin_s * num_bins + bin_v\n",
    "\n",
    "                # Histogram per chunk\n",
    "                hist = np.bincount(combined.ravel(), minlength=num_bins**3)\n",
    "                features.extend(hist)\n",
    "\n",
    "        binned_features.append(features)\n",
    "\n",
    "    # Create feature names\n",
    "    col_names = []\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            for k in range(num_bins**3):\n",
    "                col_names.append(f\"HSV_{i}_{j}_bin_{k}\")\n",
    "\n",
    "    return pd.DataFrame(binned_features, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features from images\n",
    "def add_features(dataset, df):\n",
    "    images = load_images(dataset[\"image_path\"].values)\n",
    "\n",
    "    # Get binned 3D histogram of Luma, R-G, B-Y\n",
    "    # color_contrast = get_color_contrast(images)\n",
    "    \n",
    "    # Get binned 3D histogram of HSV\n",
    "    color_contrast = get_hsv_features(images)\n",
    "    color_contrast_binned = joint_bin_features_hsv(color_contrast)\n",
    "\n",
    "    df = pd.concat([df, color_contrast_binned], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from termcolor import colored\n",
    "\n",
    "N_SPLITS = 10\n",
    "\n",
    "def train_evaluate(model, train_df):\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "    X = train_df.drop([\"ClassId\"], axis=1)\n",
    "    y = train_df[\"ClassId\"]\n",
    "\n",
    "    accuracies = []\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_val = X.iloc[train_index],  X.iloc[test_index]\n",
    "        y_train, y_val = y.iloc[train_index],  y.iloc[test_index]\n",
    "\n",
    "        # Train using sklearn\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        # Print fold results\n",
    "        print(f\"{'Fold':<6} {fold + 1:<4} | Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(colored(f\"Average Accuracy: {sum(accuracies)/len(accuracies):.4f}\", \"cyan\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# models = [\n",
    "#     RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42, n_jobs=-1), \n",
    "#     LogisticRegression(max_iter=5000),\n",
    "#     KNeighborsClassifier(n_neighbors=1, n_jobs=-1),\n",
    "# ]\n",
    "\n",
    "# # Add additional features\n",
    "# train_df = add_features(train_dataset, train_df)\n",
    "# test_df = add_features(test_dataset, test_df)\n",
    "\n",
    "# for model in models:\n",
    "#     print(colored(f\"Training {model.__class__.__name__}\", \"yellow\"))\n",
    "\n",
    "#     train_evaluate(model, train_df)\n",
    "\n",
    "#     # Retain on the entire training set\n",
    "#     X, y = train_df.drop([\"ClassId\"], axis=1), train_df[\"ClassId\"]\n",
    "#     model.fit(X, y)\n",
    "\n",
    "#     # Get accuracy on the test set\n",
    "#     # Do it here so don't need to submit to Kaggle\n",
    "#     y_pred = model.predict(test_df)\n",
    "#     test_set_benchmark = pd.read_csv(\"test_100.csv\")\n",
    "#     labels = test_set_benchmark[\"ClassId\"].values\n",
    "\n",
    "#     accuracy = accuracy_score(labels, y_pred)\n",
    "\n",
    "#     print(colored(f\"Test Set Accuracy: {accuracy:.4f}\", \"green\"))\n",
    "#     print(colored(f\"Finished training {model.__class__.__name__}\", \"yellow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mTuning RandomForestClassifier\u001b[0m\n",
      "Best parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 426}\n",
      "\u001b[33mEvaluating RandomForestClassifier on validation set\u001b[0m\n",
      "\u001b[32mValidation accuracy: 0.8716\u001b[0m\n",
      "\u001b[33mTuning LogisticRegression\u001b[0m\n",
      "Best parameters: {'C': 0.14445251022763064, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\u001b[33mEvaluating LogisticRegression on validation set\u001b[0m\n",
      "\u001b[32mValidation accuracy: 0.8925\u001b[0m\n",
      "\u001b[33mTuning KNeighborsClassifier\u001b[0m\n",
      "Best parameters: {'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n",
      "\u001b[33mEvaluating KNeighborsClassifier on validation set\u001b[0m\n",
      "\u001b[32mValidation accuracy: 0.8179\u001b[0m\n",
      "\u001b[33mTuning SVC\u001b[0m\n",
      "Best parameters: {'C': 0.033205591037519584, 'gamma': 'auto', 'kernel': 'linear'}\n",
      "\u001b[33mEvaluating SVC on validation set\u001b[0m\n",
      "\u001b[32mValidation accuracy: 0.9126\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform, randint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(random_state=42), \n",
    "    LogisticRegression(solver=\"saga\", penalty=\"l2\", max_iter=5000),\n",
    "    KNeighborsClassifier(),\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "parameter_ranges = [\n",
    "    # Random forest\n",
    "    {\n",
    "        \"n_estimators\": randint(100, 500),\n",
    "        \"max_depth\": [10, 20, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    },\n",
    "\n",
    "    # Logistic regression\n",
    "    # Values are chosen using the resources below as reference\n",
    "    # https://medium.com/@agrawalsam1997/regularization-in-logistic-regression-3d854e79f07d\n",
    "    # https://nachi-keta.medium.com/how-would-you-go-about-choosing-the-right-parameters-for-logistic-regression-9284fc4b560\n",
    "    {\n",
    "        \"C\": loguniform(1e-3, 1e2),\n",
    "        \"solver\": [\"lbfgs\", \"saga\"],\n",
    "        \"penalty\": [\"l2\"],\n",
    "    },\n",
    "    \n",
    "    # KNN\n",
    "    {\n",
    "        \"n_neighbors\": randint(1, 10),\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"p\": [1, 2],\n",
    "    },\n",
    "\n",
    "    # SVM, used the resource below as a starting point\n",
    "    # https://www.geeksforgeeks.org/rbf-svm-parameters-in-scikit-learn/\n",
    "    {\n",
    "        \"C\": loguniform(1e-3, 1e2),\n",
    "        \"kernel\": [\"linear\", \"rbf\"],\n",
    "        \"gamma\": [\"scale\", \"auto\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add additional features\n",
    "# train_df = add_features(train_dataset, train_df)\n",
    "# test_df = add_features(test_dataset, test_df)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    # Random holdout\n",
    "    X = train_df.drop(\"ClassId\", axis=1)\n",
    "    y = train_df[\"ClassId\"]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    # Must scale features for distance based models\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Tuning using CV\n",
    "    print(colored(f\"Tuning {model.__class__.__name__}\", \"yellow\"))\n",
    "\n",
    "    optimiser = RandomizedSearchCV(model, parameter_ranges[i], n_iter=20, cv=5, scoring=\"accuracy\", random_state=42, n_jobs=-1)\n",
    "    optimiser.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(\"Best parameters:\", optimiser.best_params_)\n",
    "\n",
    "    # Predict on validation set\n",
    "    print(colored(f\"Evaluating {model.__class__.__name__} on validation set\", \"yellow\"))\n",
    "\n",
    "    best_model = optimiser.best_estimator_\n",
    "    y_pred = best_model.predict(X_val_scaled)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    print(colored(f\"Validation accuracy: {accuracy:.4f}\", \"green\"))\n",
    "\n",
    "    # Predict on test set (instead of submitting to Kaggle)\n",
    "    # This is only used for sanity checks during report writing\n",
    "    # The test set if just the best CNN predictions, since we achieved a high accuracy in that\n",
    "\n",
    "    # test_df_scaled = scaler.transform(test_df)\n",
    "    # y_pred = best_model.predict(test_df_scaled)\n",
    "    # test_set_benchmark = pd.read_csv(\"test_100.csv\")\n",
    "    # labels = test_set_benchmark[\"ClassId\"].values\n",
    "    # accuracy = accuracy_score(labels, y_pred)\n",
    "\n",
    "    # print(colored(f\"Test set accuracy: {accuracy:.4f}\", \"green\"))\n",
    "    # print(colored(f\"Finished training {model.__class__.__name__}\", \"yellow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.88889\n"
     ]
    }
   ],
   "source": [
    "# The code below is just pulled form my partner's notebook, but with parameters changed based on the tuning\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_estimators = [\n",
    "    # Random forest\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "        max_depth = None, max_features = \"sqrt\", min_samples_leaf = 1, min_samples_split = 2, n_estimators=426, n_jobs=-1, random_state=42\n",
    "    )),\n",
    "\n",
    "    # SVM\n",
    "    (\"svm_rbf\", make_pipeline(SVC(\n",
    "        kernel=\"linear\", C=0.033205591037519584, gamma=\"auto\", probability=True, random_state=42\n",
    "    ))),\n",
    "\n",
    "    # Logistic regression\n",
    "    (\"lr\", make_pipeline(StandardScaler(), LogisticRegression(\n",
    "      solver=\"lbfgs\", penalty=\"l2\", max_iter=5000, C = 0.14445251022763064,\n",
    "    )))\n",
    "]\n",
    "\n",
    "meta_learner = RandomForestClassifier(\n",
    "    n_estimators=500, max_depth=10, n_jobs=-1, random_state=42, class_weight=\"balanced_subsample\"\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "stacking = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    # cv=cv,\n",
    "    n_jobs=-1,\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "\n",
    "X = train_df.drop(\"ClassId\", axis=1).values\n",
    "y = train_df[\"ClassId\"].values\n",
    "\n",
    "# Random holdout\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Fit on training data\n",
    "stacking.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Validation\n",
    "val_pred = stacking.predict(X_val_scaled)\n",
    "val_acc = accuracy_score(y_val, val_pred)\n",
    "print(f\"Validation accuracy: {val_acc:.5f}\")\n",
    "\n",
    "# # Prepare test set\n",
    "# X_test_scaled = scaler.transform(test_df)\n",
    "# y_test_true = test_set_benchmark[\"ClassId\"].values\n",
    "\n",
    "# # Retrain on all data and fit on test set\n",
    "# stacking.fit(X_train_scaled, y_train)\n",
    "# test_pred = stacking.predict(X_test_scaled)\n",
    "# test_acc = accuracy_score(y_test_true, test_pred)\n",
    "# print(f\"Test accuracy: {test_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions to the test set\n",
    "# Again, this is only for sanity checks\n",
    "# The test set if just the best CNN predictions, since we achieved a high accuracy in that\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# matrix = confusion_matrix(y_test_true, test_preds)\n",
    "\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"True\")\n",
    "# plt.title(\"Confusion Matrix Heatmap\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
